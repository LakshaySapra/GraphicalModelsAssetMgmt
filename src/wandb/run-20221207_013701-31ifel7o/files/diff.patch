diff --git a/src/GCN_LSTM.py b/src/GCN_LSTM.py
index 0110068..cf29c47 100644
--- a/src/GCN_LSTM.py
+++ b/src/GCN_LSTM.py
@@ -11,6 +11,7 @@ class GCN_LSTM(pl.LightningModule):#(nn.Module):
     def __init__(self, LSTM_input_size:int, LSTM_output_size:int=16, LSTM_num_layers:int=2, GCN_sizes:Union[None, List[int]]=None,
                  Linear_Sizes:Union[None, List[int]]=None, dropout=0.1, debug=lambda x: print(x)) -> None:
         super().__init__()
+        self.save_hyperparameters()
         
         if GCN_sizes is None:
             GCN_sizes = [16, 16]
diff --git a/src/GCN_LSTM_without_batching.py b/src/GCN_LSTM_without_batching.py
index 5a8f096..250da20 100644
--- a/src/GCN_LSTM_without_batching.py
+++ b/src/GCN_LSTM_without_batching.py
@@ -11,6 +11,7 @@ class GCN_LSTM(pl.LightningModule):#(nn.Module):
     def __init__(self, LSTM_input_size:int, LSTM_output_size:int=16, LSTM_num_layers:int=2, GCN_sizes:Union[None, List[int]]=None,
                  Linear_Sizes:Union[None, List[int]]=None, dropout=0.1, debug=lambda x: print(x)) -> None:
         super().__init__()
+        self.save_hyperparameters()
         
         if GCN_sizes is None:
             GCN_sizes = [16, 16]
@@ -69,7 +70,7 @@ class GCN_LSTM(pl.LightningModule):#(nn.Module):
             #node_features_stock = F.leaky_relu(i(node_features_stock, new_edge_index_stock, batch=batch_vector))
             #self.debug((node_features_stock.shape, new_edge_index_stock.shape, new_edge_index_stock.max().item()))
             node_features_stock = F.leaky_relu(i(node_features_stock, new_edge_index_stock))
-            print('c', (torch.isnan(node_features_stock.view(time,n_stocks,-1)).sum(dim=(0, 2)) > 0.5).sum())
+            #print('c', (torch.isnan(node_features_stock.view(time,n_stocks,-1)).sum(dim=(0, 2)) > 0.5).sum())
         
         self.debug('d')
         node_features_supplies_to = node_features
@@ -77,20 +78,21 @@ class GCN_LSTM(pl.LightningModule):#(nn.Module):
             #node_features_supplies_to = F.leaky_relu(i(node_features_supplies_to, new_edge_index_supplies_to, batch=batch_vector))
             #self.debug((node_features_supplies_to.shape, new_edge_index_supplies_to.shape))
             node_features_supplies_to = F.leaky_relu(i(node_features_supplies_to, new_edge_index_supplies_to))
-            print('f', (torch.isnan(node_features_supplies_to.view(time,n_stocks,-1)).sum(dim=(0, 2)) > 0.5).sum())
+            #print('d', (torch.isnan(node_features_supplies_to.view(time,n_stocks,-1)).sum(dim=(0, 2)) > 0.5).sum())
         
         self.debug('e')
         node_features_supplies_from = node_features
         for i in self.conv_supplies_from:
             #node_features_supplies_from = F.leaky_relu(i(node_features_supplies_from, new_edge_index_supplies_from, batch=batch_vector))
             node_features_supplies_from = F.leaky_relu(i(node_features_supplies_from, new_edge_index_supplies_from))
-            print('e', (torch.isnan(node_features_supplies_from.view(time,n_stocks,-1)).sum(dim=(0, 2)) > 0.5).sum())
+            #print('e', (torch.isnan(node_features_supplies_from.view(time,n_stocks,-1)).sum(dim=(0, 2)) > 0.5).sum())
         
         self.debug('f')
         out = torch.cat([node_features_stock, node_features_supplies_to, node_features_supplies_from], 1)
         
         self.debug('g')
         out = torch.squeeze(self.linear(out))
+        #print('g', (torch.isnan(out.view(time,n_stocks,-1)).sum(dim=(0, 2)) > 0.5).sum())
         
         #currently, shape batch*time*n_stocks. convert to batch, n_stocks, time
         return out.reshape(time, n_stocks).T
@@ -106,12 +108,10 @@ class GCN_LSTM(pl.LightningModule):#(nn.Module):
         sector_edge_lst = sector_edge_lst.squeeze(dim=0)
         c_edge_lst = c_edge_lst.squeeze(dim=0)
         s_edge_lst = s_edge_lst.squeeze(dim=0)
-        cur_hist_ret_df = cur_hist_ret_df.squeeze(dim=0)
+        cur_hist_ret_df = torch.nan_to_num(cur_hist_ret_df).squeeze(dim=0)
         cur_weekly_ret_df = cur_weekly_ret_df.squeeze(dim=0)
         mask = mask.squeeze(dim=0)[:, None] > 0.5
         
-        null_values = torch.logical_not(mask).squeeze().nonzero(as_tuple=True)
-        
         #cur_hist_ret_df = torch.nan_to_num(cur_hist_ret_df, nan=-float('inf'))
         predicted_rets = self.forward(cur_hist_ret_df, sector_edge_lst, s_edge_lst, c_edge_lst)
         time = predicted_rets.shape[1]
diff --git a/src/__pycache__/GCN_LSTM_without_batching.cpython-310.pyc b/src/__pycache__/GCN_LSTM_without_batching.cpython-310.pyc
index e95b70e..fdae2d4 100644
Binary files a/src/__pycache__/GCN_LSTM_without_batching.cpython-310.pyc and b/src/__pycache__/GCN_LSTM_without_batching.cpython-310.pyc differ
diff --git a/src/__pycache__/dataset.cpython-310.pyc b/src/__pycache__/dataset.cpython-310.pyc
index 888f9e4..59cbd42 100644
Binary files a/src/__pycache__/dataset.cpython-310.pyc and b/src/__pycache__/dataset.cpython-310.pyc differ
diff --git a/src/output.txt b/src/output.txt
index f516c4f..405b69f 100644
--- a/src/output.txt
+++ b/src/output.txt
@@ -1,3 +1 @@
-tensor([[194, 194, 194,  ...,  61,  61, 250],
-        [325, 125, 277,  ..., 250,  68,  68]]) 0
-Training: 0it [00:00, ?it/s]Training:   0%|          | 0/469 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/469 [00:00<?, ?it/s] Epoch 0:   0%|          | 0/469 [00:00<?, ?it/s]
+Training: 0it [00:00, ?it/s]Training:   0%|          | 0/469 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/469 [00:00<?, ?it/s] Epoch 0:   0%|          | 1/469 [00:01<12:10,  1.56s/it]Epoch 0:   0%|          | 1/469 [00:01<12:11,  1.56s/it, loss=2.44e+03, v_num=z5lg]Epoch 0:   0%|          | 2/469 [00:02<09:12,  1.18s/it, loss=2.44e+03, v_num=z5lg]Epoch 0:   0%|          | 2/469 [00:02<09:16,  1.19s/it, loss=2.34e+03, v_num=z5lg]Epoch 0:   1%|          | 3/469 [00:02<07:45,  1.00it/s, loss=2.34e+03, v_num=z5lg]Epoch 0:   1%|          | 3/469 [00:03<07:48,  1.01s/it, loss=2.34e+03, v_num=z5lg]Epoch 0:   1%|          | 4/469 [00:03<06:47,  1.14it/s, loss=2.34e+03, v_num=z5lg]Epoch 0:   1%|          | 4/469 [00:03<06:49,  1.14it/s, loss=2.35e+03, v_num=z5lg]Epoch 0:   1%|          | 5/469 [00:04<06:20,  1.22it/s, loss=2.35e+03, v_num=z5lg]Epoch 0:   1%|          | 5/469 [00:04<06:22,  1.21it/s, loss=2.33e+03, v_num=z5lg]Epoch 0:   1%|▏         | 6/469 [00:04<06:00,  1.28it/s, loss=2.33e+03, v_num=z5lg]Epoch 0:   1%|▏         | 6/469 [00:04<06:02,  1.28it/s, loss=2.33e+03, v_num=z5lg]Epoch 0:   1%|▏         | 7/469 [00:05<05:46,  1.34it/s, loss=2.33e+03, v_num=z5lg]Epoch 0:   1%|▏         | 7/469 [00:05<05:48,  1.32it/s, loss=2.33e+03, v_num=z5lg]Epoch 0:   2%|▏         | 8/469 [00:05<05:36,  1.37it/s, loss=2.33e+03, v_num=z5lg]Epoch 0:   2%|▏         | 8/469 [00:05<05:38,  1.36it/s, loss=2.31e+03, v_num=z5lg]Epoch 0:   2%|▏         | 9/469 [00:06<05:30,  1.39it/s, loss=2.31e+03, v_num=z5lg]Epoch 0:   2%|▏         | 9/469 [00:06<05:32,  1.38it/s, loss=2.29e+03, v_num=z5lg]Epoch 0:   2%|▏         | 10/469 [00:07<05:25,  1.41it/s, loss=2.29e+03, v_num=z5lg]Epoch 0:   2%|▏         | 10/469 [00:07<05:26,  1.41it/s, loss=2.28e+03, v_num=z5lg]Epoch 0:   2%|▏         | 11/469 [00:07<05:23,  1.42it/s, loss=2.28e+03, v_num=z5lg]Epoch 0:   2%|▏         | 11/469 [00:07<05:24,  1.41it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 12/469 [00:08<05:18,  1.44it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 12/469 [00:08<05:18,  1.43it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 13/469 [00:08<05:11,  1.47it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 13/469 [00:08<05:11,  1.46it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 14/469 [00:09<05:04,  1.50it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 14/469 [00:09<05:04,  1.49it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 15/469 [00:09<04:59,  1.51it/s, loss=2.25e+03, v_num=z5lg]Epoch 0:   3%|▎         | 15/469 [00:09<05:00,  1.51it/s, loss=2.23e+03, v_num=z5lg]Epoch 0:   3%|▎         | 16/469 [00:10<04:56,  1.53it/s, loss=2.23e+03, v_num=z5lg]Epoch 0:   3%|▎         | 16/469 [00:10<04:57,  1.52it/s, loss=2.22e+03, v_num=z5lg]Epoch 0:   4%|▎         | 17/469 [00:11<04:53,  1.54it/s, loss=2.22e+03, v_num=z5lg]Epoch 0:   4%|▎         | 17/469 [00:11<04:54,  1.54it/s, loss=2.21e+03, v_num=z5lg]Epoch 0:   4%|▍         | 18/469 [00:11<04:49,  1.56it/s, loss=2.21e+03, v_num=z5lg]Epoch 0:   4%|▍         | 18/469 [00:11<04:50,  1.55it/s, loss=2.21e+03, v_num=z5lg]Epoch 0:   4%|▍         | 19/469 [00:12<04:45,  1.57it/s, loss=2.21e+03, v_num=z5lg]Epoch 0:   4%|▍         | 19/469 [00:12<04:46,  1.57it/s, loss=2.2e+03, v_num=z5lg] Epoch 0:   4%|▍         | 20/469 [00:12<04:44,  1.58it/s, loss=2.2e+03, v_num=z5lg]Epoch 0:   4%|▍         | 20/469 [00:12<04:45,  1.57it/s, loss=2.2e+03, v_num=z5lg]Epoch 0:   4%|▍         | 21/469 [00:13<04:43,  1.58it/s, loss=2.2e+03, v_num=z5lg]Epoch 0:   4%|▍         | 21/469 [00:13<04:44,  1.57it/s, loss=2.17e+03, v_num=z5lg]Epoch 0:   5%|▍         | 22/469 [00:13<04:41,  1.59it/s, loss=2.17e+03, v_num=z5lg]Epoch 0:   5%|▍         | 22/469 [00:13<04:42,  1.58it/s, loss=2.17e+03, v_num=z5lg]Epoch 0:   5%|▍         | 23/469 [00:14<04:38,  1.60it/s, loss=2.17e+03, v_num=z5lg]Epoch 0:   5%|▍         | 23/469 [00:14<04:39,  1.59it/s, loss=2.15e+03, v_num=z5lg]Epoch 0:   5%|▌         | 24/469 [00:14<04:37,  1.60it/s, loss=2.15e+03, v_num=z5lg]Epoch 0:   5%|▌         | 24/469 [00:15<04:38,  1.60it/s, loss=2.13e+03, v_num=z5lg]Epoch 0:   5%|▌         | 25/469 [00:15<04:35,  1.61it/s, loss=2.13e+03, v_num=z5lg]Epoch 0:   5%|▌         | 25/469 [00:15<04:35,  1.61it/s, loss=2.11e+03, v_num=z5lg]Epoch 0:   6%|▌         | 26/469 [00:16<04:34,  1.61it/s, loss=2.11e+03, v_num=z5lg]Epoch 0:   6%|▌         | 26/469 [00:16<04:34,  1.61it/s, loss=2.1e+03, v_num=z5lg] Epoch 0:   6%|▌         | 27/469 [00:16<04:33,  1.62it/s, loss=2.1e+03, v_num=z5lg]Epoch 0:   6%|▌         | 27/469 [00:16<04:34,  1.61it/s, loss=2.07e+03, v_num=z5lg]Epoch 0:   6%|▌         | 28/469 [00:17<04:31,  1.62it/s, loss=2.07e+03, v_num=z5lg]Epoch 0:   6%|▌         | 28/469 [00:17<04:32,  1.62it/s, loss=2.07e+03, v_num=z5lg]Epoch 0:   6%|▌         | 29/469 [00:17<04:32,  1.62it/s, loss=2.07e+03, v_num=z5lg]Epoch 0:   6%|▌         | 29/469 [00:17<04:32,  1.61it/s, loss=2.05e+03, v_num=z5lg]Epoch 0:   6%|▋         | 30/469 [00:18<04:31,  1.62it/s, loss=2.05e+03, v_num=z5lg]Epoch 0:   6%|▋         | 30/469 [00:18<04:32,  1.61it/s, loss=2.04e+03, v_num=z5lg]Epoch 0:   7%|▋         | 31/469 [00:19<04:30,  1.62it/s, loss=2.04e+03, v_num=z5lg]Epoch 0:   7%|▋         | 31/469 [00:19<04:31,  1.61it/s, loss=2.03e+03, v_num=z5lg]Epoch 0:   7%|▋         | 32/469 [00:19<04:29,  1.62it/s, loss=2.03e+03, v_num=z5lg]Epoch 0:   7%|▋         | 32/469 [00:19<04:30,  1.62it/s, loss=2.02e+03, v_num=z5lg]Epoch 0:   7%|▋         | 33/469 [00:20<04:28,  1.62it/s, loss=2.02e+03, v_num=z5lg]Epoch 0:   7%|▋         | 33/469 [00:20<04:29,  1.62it/s, loss=2e+03, v_num=z5lg]   Epoch 0:   7%|▋         | 34/469 [00:20<04:27,  1.63it/s, loss=2e+03, v_num=z5lg]Epoch 0:   7%|▋         | 34/469 [00:20<04:28,  1.62it/s, loss=1.98e+03, v_num=z5lg]Epoch 0:   7%|▋         | 35/469 [00:21<04:27,  1.62it/s, loss=1.98e+03, v_num=z5lg]Epoch 0:   7%|▋         | 35/469 [00:21<04:27,  1.62it/s, loss=1.97e+03, v_num=z5lg]Epoch 0:   8%|▊         | 36/469 [00:22<04:27,  1.62it/s, loss=1.97e+03, v_num=z5lg]Epoch 0:   8%|▊         | 36/469 [00:22<04:28,  1.62it/s, loss=1.95e+03, v_num=z5lg]Epoch 0:   8%|▊         | 37/469 [00:22<04:26,  1.62it/s, loss=1.95e+03, v_num=z5lg]Epoch 0:   8%|▊         | 37/469 [00:22<04:26,  1.62it/s, loss=1.94e+03, v_num=z5lg]Epoch 0:   8%|▊         | 38/469 [00:23<04:25,  1.62it/s, loss=1.94e+03, v_num=z5lg]Epoch 0:   8%|▊         | 38/469 [00:23<04:26,  1.62it/s, loss=1.93e+03, v_num=z5lg]Epoch 0:   8%|▊         | 39/469 [00:24<04:26,  1.61it/s, loss=1.93e+03, v_num=z5lg]Epoch 0:   8%|▊         | 39/469 [00:24<04:27,  1.61it/s, loss=1.92e+03, v_num=z5lg]Epoch 0:   9%|▊         | 40/469 [00:24<04:26,  1.61it/s, loss=1.92e+03, v_num=z5lg]Epoch 0:   9%|▊         | 40/469 [00:24<04:26,  1.61it/s, loss=1.9e+03, v_num=z5lg] Epoch 0:   9%|▊         | 41/469 [00:25<04:26,  1.61it/s, loss=1.9e+03, v_num=z5lg]Epoch 0:   9%|▊         | 41/469 [00:25<04:26,  1.61it/s, loss=1.89e+03, v_num=z5lg]Epoch 0:   9%|▉         | 42/469 [00:25<04:24,  1.62it/s, loss=1.89e+03, v_num=z5lg]Epoch 0:   9%|▉         | 42/469 [00:25<04:24,  1.62it/s, loss=1.88e+03, v_num=z5lg]Epoch 0:   9%|▉         | 43/469 [00:26<04:23,  1.62it/s, loss=1.88e+03, v_num=z5lg]Epoch 0:   9%|▉         | 43/469 [00:26<04:23,  1.62it/s, loss=1.86e+03, v_num=z5lg]Epoch 0:   9%|▉         | 44/469 [00:27<04:22,  1.62it/s, loss=1.86e+03, v_num=z5lg]Epoch 0:   9%|▉         | 44/469 [00:27<04:23,  1.61it/s, loss=1.85e+03, v_num=z5lg]Epoch 0:  10%|▉         | 45/469 [00:27<04:22,  1.61it/s, loss=1.85e+03, v_num=z5lg]Epoch 0:  10%|▉         | 45/469 [00:27<04:23,  1.61it/s, loss=1.84e+03, v_num=z5lg]Epoch 0:  10%|▉         | 46/469 [00:28<04:22,  1.61it/s, loss=1.84e+03, v_num=z5lg]Epoch 0:  10%|▉         | 46/469 [00:28<04:22,  1.61it/s, loss=1.82e+03, v_num=z5lg]Epoch 0:  10%|█         | 47/469 [00:29<04:22,  1.61it/s, loss=1.82e+03, v_num=z5lg]Epoch 0:  10%|█         | 47/469 [00:29<04:23,  1.60it/s, loss=1.81e+03, v_num=z5lg]Epoch 0:  10%|█         | 48/469 [00:29<04:22,  1.60it/s, loss=1.81e+03, v_num=z5lg]Epoch 0:  10%|█         | 48/469 [00:30<04:23,  1.60it/s, loss=1.79e+03, v_num=z5lg]Epoch 0:  10%|█         | 49/469 [00:30<04:22,  1.60it/s, loss=1.79e+03, v_num=z5lg]Epoch 0:  10%|█         | 49/469 [00:30<04:22,  1.60it/s, loss=1.78e+03, v_num=z5lg]Epoch 0:  11%|█         | 50/469 [00:31<04:21,  1.60it/s, loss=1.78e+03, v_num=z5lg]Epoch 0:  11%|█         | 50/469 [00:31<04:21,  1.60it/s, loss=1.77e+03, v_num=z5lg]Epoch 0:  11%|█         | 51/469 [00:31<04:20,  1.60it/s, loss=1.77e+03, v_num=z5lg]Epoch 0:  11%|█         | 51/469 [00:31<04:21,  1.60it/s, loss=1.77e+03, v_num=z5lg]Epoch 0:  11%|█         | 51/469 [00:33<04:31,  1.54it/s, loss=1.77e+03, v_num=z5lg]
\ No newline at end of file
diff --git a/src/train.py b/src/train.py
index 268cda8..4eaab9b 100644
--- a/src/train.py
+++ b/src/train.py
@@ -4,20 +4,16 @@ from GCN_LSTM_without_batching import GCN_LSTM
 import torch
 from tqdm.auto import tqdm
 import pytorch_lightning as pl
+from pytorch_lightning.loggers import WandbLogger
 
 if __name__ == '__main__':
     dataset = GNNDataset()
-    for i in tqdm(iter(dataset)):
-        pass
-    #x = next(iter(dataset))
-    #print(x[0], torch.isnan(x[0]).sum().item())
-    '''
     dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=False, persistent_workers=False) #takes care of shuffling
     model = GCN_LSTM(13, debug=lambda x: None)
-    trainer = pl.Trainer(gpus=1, max_epochs=1)
+    wandb_logger = WandbLogger(project='PGM Project', log_model="all")
+    trainer = pl.Trainer(gpus=1, max_epochs=1, logger=wandb_logger)
     #trainer = pl.Trainer(max_epochs=1)
     trainer.fit(model=model, train_dataloaders=dataloader)
-    '''
 
 # Notes
 # Getting PyG to work:
